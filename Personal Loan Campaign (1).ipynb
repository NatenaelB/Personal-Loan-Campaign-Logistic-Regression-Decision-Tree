{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AllLife Bank Personal Loan Campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "AllLife Bank is a US bank that has a growing customer base. The majority of these customers are liability customers (depositors) with varying sizes of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors).\n",
    "\n",
    "A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio.\n",
    "\n",
    "You as a Data scientist at AllLife bank have to build a model that will help the marketing department to identify the potential customers who have a higher probability of purchasing the loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "* To predict whether a liability customer will buy personal loans.\n",
    "* Which variables are most significant.\n",
    "* Which segment of customers should be targeted more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "* ID: Customer ID\n",
    "* Age: Customerâ€™s age in completed years\n",
    "* Experience: #years of professional experience\n",
    "* Income: Annual income of the customer (in thousand dollars)\n",
    "* ZIP Code: Home Address ZIP code.\n",
    "* Family: the Family size of the customer\n",
    "* CCAvg: Average spending on credit cards per month (in thousand dollars)\n",
    "* Education: Education Level. 1: Undergrad; 2: Graduate;3: Advanced/Professional\n",
    "* Mortgage: Value of house mortgage if any. (in thousand dollars)\n",
    "* Personal_Loan: Did this customer accept the personal loan offered in the last campaign? (0: No, 1: Yes)\n",
    "* Securities_Account: Does the customer have securities account with the bank? (0: No, 1: Yes)\n",
    "* CD_Account: Does the customer have a certificate of deposit (CD) account with the bank? (0: No, 1: Yes)\n",
    "* Online: Do customers use internet banking facilities? (0: No, 1: Yes)\n",
    "* CreditCard: Does the customer use a credit card issued by any other Bank (excluding All life Bank)? (0: No, 1: Yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_46468/2666472089.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m# For pandas profiling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas_profiling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas_profiling'"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# this will help in making the Python code more structured automatically (good coding practice)\\n%load_ext nb_black\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\\n\\nwarnings.simplefilter(\\\"ignore\\\", ConvergenceWarning)\\n\\n# Libraries to help with reading and manipulating data\\n\\nimport pandas as pd\\nimport numpy as np\\n\\n# Library to split data\\nfrom sklearn.model_selection import train_test_split\\n\\n# libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Removes the limit for the number of displayed columns\\npd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\npd.set_option(\\\"display.max_rows\\\", 200)\\n\\n\\n# To build model for prediction\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn import tree\\n\\n# To tune different models\\nfrom sklearn.model_selection import GridSearchCV\\n\\n\\n# To get diferent metric scores\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    plot_confusion_matrix,\\n    precision_recall_curve,\\n    roc_curve,\\n    make_scorer,\\n)\\n\\n# For pandas profiling\\nfrom pandas_profiling import ProfileReport\";\n",
       "                var nbb_formatted_code = \"# this will help in making the Python code more structured automatically (good coding practice)\\n%load_ext nb_black\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\\n\\nwarnings.simplefilter(\\\"ignore\\\", ConvergenceWarning)\\n\\n# Libraries to help with reading and manipulating data\\n\\nimport pandas as pd\\nimport numpy as np\\n\\n# Library to split data\\nfrom sklearn.model_selection import train_test_split\\n\\n# libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Removes the limit for the number of displayed columns\\npd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\npd.set_option(\\\"display.max_rows\\\", 200)\\n\\n\\n# To build model for prediction\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn import tree\\n\\n# To tune different models\\nfrom sklearn.model_selection import GridSearchCV\\n\\n\\n# To get diferent metric scores\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    plot_confusion_matrix,\\n    precision_recall_curve,\\n    roc_curve,\\n    make_scorer,\\n)\\n\\n# For pandas profiling\\nfrom pandas_profiling import ProfileReport\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this will help in making the Python code more structured automatically (good coding practice)\n",
    "%load_ext nb_black\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
    "\n",
    "# Libraries to help with reading and manipulating data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Library to split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# libaries to help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Removes the limit for the number of displayed columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Sets the limit for the number of displayed rows\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "\n",
    "# To build model for prediction\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# To tune different models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# To get diferent metric scores\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    plot_confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    make_scorer,\n",
    ")\n",
    "#pip install pandas_profiling\n",
    "# For pandas profiling\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loan = pd.read_csv(\"Loan_Modelling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying data to another variable to avoid any changes to original data\n",
    "data = Loan.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the first and last 5 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dEuSu6e4hU2e"
   },
   "source": [
    "### Understand the shape of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The dataset has 5000 rows and 14 columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwAu-vEwhU2m"
   },
   "source": [
    "### Check the data types of the columns for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are no null values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting the data type of categorical features to 'category'\n",
    "cat_cols = [\n",
    "    \"Education\",\n",
    "    \"Personal_Loan\",\n",
    "    \"Securities_Account\",\n",
    "    \"CD_Account\",\n",
    "    \"Online\",\n",
    "    \"CreditCard\",\n",
    "]\n",
    "data[cat_cols] = data[cat_cols].astype(\"category\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7E1orwlJhU20"
   },
   "source": [
    "### Summary of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `ID`: The ID attribute does not add any information to our analysis as all the values are unique. There is no association between a person's customer ID and loan, also it does not provide any general conclusion for future potential loan customers. We can neglect this information for our model prediction.\n",
    "* `Age`: Average age of customers is 45 years, age of customers has a wide range from 23 to 67 years. \n",
    "* `Experience`: A negative experience of -3 seems to be a data entry error. The average working experience of the customers is ~20 years, we have some highly experienced working professions in the data.\n",
    "* `Income`: Average income of customers is 73k dollars. Income has a wide range from 8k dollars to 224k dollars, there's also a very huge difference in 75th percentile and maximum value which indicates there might be outliers present in the data. \n",
    "* `ZIPCode`: ZIPCode seems to have many unique values, we will see if some insights can be extracted from it.\n",
    "* `Family`:  75% of the customers have 3 or less than 3 dependents.\n",
    "* `CCAvg`: Some customers have average spending of 0 dollars a month. There's a huge difference in the 75th percentile and maximum value of the average spendings indicating that there might be outliers present.\n",
    "* `Mortgage`: Average mortgage value of house of customers is ~56k dollars. Many customers do not have any mortgages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"ID\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=[\"category\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in cat_cols:\n",
    "    print(\"Unique values in\", i, \"are :\")\n",
    "    print(data[i].value_counts())\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Education: Most of the customers are Undergraduates.\n",
    "* Personal_Loan: Most of the customers didn't accept the loan in the previous campaign.\n",
    "* Securities_Account: Most of the customers do not have a Securities Account.\n",
    "* CD_Account: Most of the customers do not have a CD Account.\n",
    "* Online: Most customers do not use internet banking facilities.\n",
    "* CreditCard: Most customers do not use credit cards from any other bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
    "    \"\"\"\n",
    "    Boxplot and histogram combined\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (12,7))\n",
    "    kde: whether to show the density curve (default False)\n",
    "    bins: number of bins for histogram (default None)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  # creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
    "    sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
    "    ) if bins else sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
    "    )  # Add median to the histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(data, \"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The distribution of Age is fairly symmetrical about the mean and the median. \n",
    "* The mean and median age of customers is almost equal to ~45 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Experience\n",
    "\n",
    "* Treating the negative values of Experience: We assume that these negative signs here are data input errors, so we will replace them with positive signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"Experience\"] < 0][\"Experience\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Experience\"].replace(-1, 1, inplace=True)\n",
    "data[\"Experience\"].replace(-2, 2, inplace=True)\n",
    "data[\"Experience\"].replace(-3, 3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Experience\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Minimum value of experience in 0 now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(data, \"Experience\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Experience variable seems to be fairly symmetrical about mean and median.\n",
    "* The distribution of Experience looks like the distribution of Age, it would be interesting to see if there's a correlation between them.\n",
    "* The mean and median experience of customers is equal to ~20 years.\n",
    "* Some customers have experience of more than 40 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Observations on Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(data, \"Income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The distribution of Income is skewed to right.\n",
    "* Some customers have an Income above 200k dollars.\n",
    "* 50% of the customers have income less than 64k dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on CCAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(data, \"CCAvg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The distribution of CCAvg is skewed to the right and there are many outliers.\n",
    "* 50% of the customers have CCAvg less than 1.5.\n",
    "* We should check if the customers who spend more monthly are inclined toward taking a loan or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Mortgage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(data, \"Mortgage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most customers have not mortgaged their house but there are many outliers.\n",
    "* Some customers have a mortgage house value of more than 600k dollars.\n",
    "* Such customers can be potential customers who require a personal loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create labeled barplots\n",
    "\n",
    "\n",
    "def labeled_barplot(data, feature, perc=False, n=None):\n",
    "    \"\"\"\n",
    "    Barplot with percentage at the top\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    perc: whether to display percentages instead of count (default is False)\n",
    "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(data[feature])  # length of the column\n",
    "    count = data[feature].nunique()\n",
    "    if n is None:\n",
    "        plt.figure(figsize=(count + 1, 5))\n",
    "    else:\n",
    "        plt.figure(figsize=(n + 1, 5))\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=15)\n",
    "    ax = sns.countplot(\n",
    "        data=data,\n",
    "        x=feature,\n",
    "        palette=\"Paired\",\n",
    "        order=data[feature].value_counts().index[:n].sort_values(),\n",
    "    )\n",
    "\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )  # percentage of each class of the category\n",
    "        else:\n",
    "            label = p.get_height()  # count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
    "        y = p.get_height()  # height of the plot\n",
    "\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (x, y),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            size=12,\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )  # annotate the percentage\n",
    "\n",
    "    plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"Family\", perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most of the customers are single/live alone (~30%).\n",
    "* Second most are the ones with one dependent on them (25.9%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's map the values to 1: Undergrad; 2: Graduate 3: Advanced/Professional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Education\"].replace(1, \"Undergraduate\", inplace=True)\n",
    "data[\"Education\"].replace(2, \"Graduate\", inplace=True)\n",
    "data[\"Education\"].replace(3, \"Professional\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"Education\", perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most of the customers are undergraduate (41.9%), followed by customers who have advanced/professional education (30%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Securities_Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"Securities_Account\", perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 89% of the customers do not have a securities account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on CD_Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"CD_Account\", perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 94% of the customers do not have a certificate of deposit (CD_Account) with the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"Online\", perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Approximately 60% of the customers use internet banking facilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation on CreditCard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"CreditCard\", perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Approximately 70% of customers do not have a credit card issued by any other bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation on ZIPCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of uniques in the zip code\n",
    "data[\"ZIPCode\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 467 unique values in the zip code.\n",
    "* In the US, The first digit of a PIN indicates the zone or a region, the second indicates the sub-zone, and the third, combined with the first two, indicates the sorting district within that zone. The final three digits are assigned to individual post offices within the sorting district\n",
    "* Let's try to group them based on the first 2 digits to reduce the number of unique values in the ZIPCode column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ZIPCode\"] = data[\"ZIPCode\"].astype(str)\n",
    "print(\n",
    "    \"Number of unique values if we take first two digits of ZIPCode: \",\n",
    "    data[\"ZIPCode\"].str[0:2].nunique(),\n",
    ")\n",
    "data[\"ZIPCode\"] = data[\"ZIPCode\"].str[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"ZIPCode\", perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All the customers are from region 9 and most from sub-region 4 followed by sub-region 2.\n",
    "* This indicates that it is located in region 9 and has customers from nearby areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "beo_tDmVhU3-"
   },
   "source": [
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.heatmap(data.corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Age and Experience have a perfect correlation hence one of these variables can be dropped while model building as they will provide the same information to the model.\n",
    "\n",
    "* Income and CCAvg have a moderate correlation which makes sense as the Income increases the spendings might also increase.\n",
    "\n",
    "* Family has a negative correlation with Income, which is quite surprising as family size increases the income of the family decreases. But this correlation is not too strong to conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_barplot(data, predictor, target):\n",
    "    \"\"\"\n",
    "    Print the category counts and plot a stacked bar chart\n",
    "\n",
    "    data: dataframe\n",
    "    predictor: independent variable\n",
    "    target: target variable\n",
    "    \"\"\"\n",
    "    count = data[predictor].nunique()\n",
    "    sorter = data[target].value_counts().index[-1]\n",
    "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
    "        by=sorter, ascending=False\n",
    "    )\n",
    "    print(tab1)\n",
    "    print(\"-\" * 120)\n",
    "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
    "        by=sorter, ascending=False\n",
    "    )\n",
    "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n",
    "    plt.legend(\n",
    "        loc=\"lower left\", frameon=False,\n",
    "    )\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Personal_Loan vs Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_barplot(data, \"Education\", \"Personal_Loan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ~15% of the customers who have done advanced/professional studies are the ones that require a personal loan.\n",
    "* Undergraduates have the least requirement of personal loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal_Loan vs Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_barplot(data, \"Family\", \"Personal_Loan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As the family size increases the requirement of a Personal Loan also increases.\n",
    "* Customers who have a family size of more than 2 are more likely to take a Personal Loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal_Loan vs Securities_Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_barplot(data, \"Securities_Account\", \"Personal_Loan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's not much difference between the customers who do or do not have a Security Account and require a Personal Loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal_Loan vs CD_Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_barplot(data, \"CD_Account\", \"Personal_Loan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ~50% of the customers who have a certificate of deposit with the bank (CD_Account) are the ones that have the requirement of a Personal Loan.\n",
    "* This can be one of the significant predictors of Personal Loan as it provides good separation between two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal_Loan vs Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_barplot(data, \"Online\", \"Personal_Loan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's not much difference between the customers who do or do use internet banking facilities and require a Personal Loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal_Loan vs CreditCard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_barplot(data, \"CreditCard\", \"Personal_Loan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's not much difference between the customers who do or do use a credit card from other banks and require a Personal Loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal_Loan vs ZIPCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_barplot(data, \"ZIPCode\", \"Personal_Loan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All the sub-regions show fairly the same distribution/ requirement of a Personal Loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to plot distributions wrt target\n",
    "\n",
    "\n",
    "def distribution_plot_wrt_target(data, predictor, target):\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    target_uniq = data[target].unique()\n",
    "\n",
    "    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n",
    "    sns.histplot(\n",
    "        data=data[data[target] == target_uniq[0]],\n",
    "        x=predictor,\n",
    "        kde=True,\n",
    "        ax=axs[0, 0],\n",
    "        color=\"teal\",\n",
    "        stat=\"density\",\n",
    "    )\n",
    "\n",
    "    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n",
    "    sns.histplot(\n",
    "        data=data[data[target] == target_uniq[1]],\n",
    "        x=predictor,\n",
    "        kde=True,\n",
    "        ax=axs[0, 1],\n",
    "        color=\"orange\",\n",
    "        stat=\"density\",\n",
    "    )\n",
    "\n",
    "    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n",
    "    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n",
    "\n",
    "    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n",
    "    sns.boxplot(\n",
    "        data=data,\n",
    "        x=target,\n",
    "        y=predictor,\n",
    "        ax=axs[1, 1],\n",
    "        showfliers=False,\n",
    "        palette=\"gist_rainbow\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Loan vs Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plot_wrt_target(data, \"Age\", \"Personal_Loan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The customers who have the requirement of a Personal Loan have a wider range than the ones who do not require a Personal Loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Loan vs Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plot_wrt_target(data, \"Experience\", \"Personal_Loan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's no difference in the years of experience of customers who took Personal Loan and those who did not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Loan vs Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plot_wrt_target(data, \"Income\", \"Personal_Loan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Those customers who have an income higher than 90k-100k dollars are the potential customers who will take the Personal Loan.\n",
    "* Income seems to be a significant predictor as it provides a good separation between two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Loan vs CCAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plot_wrt_target(data, \"CCAvg\", \"Personal_Loan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The customers with a CCAvg of greater than 5 can be the potential customers who opt for Personal Loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data[[\"Income\", \"CCAvg\"]].columns.tolist()\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, variable in enumerate(cols):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    sns.lineplot(data[\"Age\"], data[variable], hue=data[\"Personal_Loan\"], ci=0)\n",
    "    plt.tight_layout()\n",
    "    plt.title(variable)\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Customers who require Personal Loans are the ones whose income fluctuates largely with the increase in Age and - This clearly shows how financial stability plays a big part in the requirement of a loan.\n",
    "Customers whose income fluctuates largely with the age i.e. have less income stability are more likely to take a personal loan.\n",
    "\n",
    "* We can observe a similar pattern like income w.r.t CCAvg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find the percentage of outliers, in each column of the data, using IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = data.quantile(0.25)  # To find the 25th percentile and 75th percentile.\n",
    "Q3 = data.quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1  # Inter Quantile Range (75th perentile - 25th percentile)\n",
    "\n",
    "lower = (\n",
    "    Q1 - 1.5 * IQR\n",
    ")  # Finding lower and upper bounds for all values. All values outside these bounds are outliers\n",
    "upper = Q3 + 1.5 * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (data.select_dtypes(include=[\"float64\", \"int64\"]) < lower)\n",
    "    | (data.select_dtypes(include=[\"float64\", \"int64\"]) > upper)\n",
    ").sum() / len(data) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After identifying outliers, we can decide whether to remove/treat them or not. It depends on one's approach, here we are not going to treat them as there will be outliers in a real case scenario (in Income, Mortgage value, Average spending on the credit card, etc) and we would want our model to learn the underlying pattern for such customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='link1'>Summary of EDA</a>\n",
    "**Data Description:**\n",
    "* The dataset has 5000 rows and 14 columns \n",
    "* There are no missing values in the dataset.\n",
    "* Dependent variable is Personal_Loan.\n",
    "\n",
    "**Data Cleaning:**\n",
    "\n",
    "* `Experience:` We observed that there are negative values in the Experience column. We assume that these negative signs here are data input errors, so we will replace them with positive values.\n",
    "* `ZIPCode`: \n",
    "    * There are 467 unique values in the zip code.\n",
    "    * In the US, The first digit of a PIN indicates the zone or a region, the second indicates the sub-zone, and the third, combined with the first two, indicates the sorting district within that zone. The final three digits are assigned to individual post offices within the sorting district\n",
    "    * We grouped them based on the first 2 digits to reduce the number of unique values in the ZIPCode column. \n",
    "* `ID`: The ID attribute does not add any information to our analysis as all the values are unique. There is no association between a person's customer ID and loan, also it does not provide any general conclusion for future potential loan customers. We will drop this feature.\n",
    "\n",
    "\n",
    "**Observations from EDA:**\n",
    "\n",
    "* `Age`: Average age of customers is 45 years, age of customers has a wide range from 23 to 67 years. The distribution of Age is fairly symmetrical about the mean and the median. \n",
    "\n",
    "\n",
    "* `Experience`: \n",
    "    * The Experience variable seems to be fairly symmetrical about mean and median. The distribution of Experience looks like the distribution of Age. The mean and median experience of customers is equal to ~20 years.\n",
    "    * Some customers have experience of more than 40 years.\n",
    "\n",
    "\n",
    "* `Income`: \n",
    "    * Average income of customers is 73k dollars. Income has a wide range from 8k dollars to 224k dollars.\n",
    "    * The distribution of Income is skewed to right.\n",
    "    * 50% of the customers have income less than 64k dollars.\n",
    "\n",
    "\n",
    "* `CCAvg`: \n",
    "    * Some customers have average spending of 0 dollars a month.\n",
    "    * The distribution of CCAvg is skewed to the right and there are many outliers.\n",
    "    * 50% of the customers have CCAvg less than 1.5.\n",
    "\n",
    "\n",
    "* `Mortgage`: \n",
    "    * Average mortgage value of house of customers is ~56k dollars\n",
    "    * Most customers have not mortgaged their house but there are many outliers.\n",
    "    * Some customers have a mortgage house value of more than 600k dollars. Such customers can be potential customers who require a personal loan.\n",
    "\n",
    "\n",
    "* `Family`: \n",
    "    * Most of the customers are single/live alone (~30%).\n",
    "    * Second most are the ones with one dependent on them (25.9%).\n",
    "\n",
    "* `ZIPCode`: \n",
    "    * All the customers are from region 9 and most from sub-region 4 followed by sub-region 2.\n",
    "    * This indicates that bank is located in region 9 and has customers from nearby areas.\n",
    "\n",
    "\n",
    "* `Education`: Most of the customers are undergraduate (41.9%), followed by customers who have advanced/professional education (30%).\n",
    "\n",
    "* `Securities_Account`:* 89% of the customers do not have a securities account.\n",
    "* `CD_Account`: 94% of the customers do not have a certificate of deposit (CD_Account) with the bank.\n",
    "\n",
    "\n",
    "* `Observations on correlation:`\n",
    "    * Age and Experience have an almost perfect correlation hence one of these variables can be dropped while model building as they will provide the same information to the model.\n",
    "    * Income and CCAvg have a moderate correlation which makes sense as the Income increases the spendings might also increase.\n",
    "\n",
    "\n",
    "* `Personal_Loan vs Education:`\n",
    "    * ~15% of the customers who have done advanced/professional studies are the ones that require a personal loan.\n",
    "    * Undergraduates have the least requirement of personal loans.\n",
    "\n",
    "\n",
    "* `Personal_Loan vs Family:`\n",
    "    * As the family size increases the requirement of a Personal Loan also increases.\n",
    "    * Customers who have a family size of more than 2 are more likely to take a Personal Loan.\n",
    "\n",
    "\n",
    "* `Personal_Loan vs CD_Account:`\n",
    "    * ~50% of the customers who have a certificate of deposit with the bank (CD_Account) are the ones that have the requirement of a Personal Loan.\n",
    "    * This can be one of the significant predictors of Personal Loan as it provides good separation between two classes.\n",
    "\n",
    "\n",
    "* `Personal_Loan vs Income:`\n",
    "    * Those customers who have an income higher than 90k-100k dollars are the potential customers who will take the Personal Loan.\n",
    "    * Income seems to be a significant predictor as it provides a good separation between two classes.\n",
    "\n",
    "\n",
    "* `Personal_Loan vs CCAvg:` The customers with a CCAvg of greater than 5 can be the potential customers who opt for Personal Loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions for data pre-processing:\n",
    "\n",
    "* We will not be treating the outliers as in a real case scenario (in Income, Mortgage value, Average spending on the credit card, etc) and we would want our model to learn the underlying pattern for such customers\n",
    "* We have 6 categorical independent variables but 4 of them are binary, so we'll have the same results with them even after creating dummies\n",
    "* So we will only make dummies for ZIPCode and Education."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ZIPCode\"] = data[\"ZIPCode\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping Experience as it is perfectly correlated with Age\n",
    "X = data.drop([\"Personal_Loan\", \"Experience\"], axis=1)\n",
    "\n",
    "Y = data[\"Personal_Loan\"]\n",
    "\n",
    "X = pd.get_dummies(X, columns=[\"ZIPCode\", \"Education\"], drop_first=True)\n",
    "\n",
    "# Splitting data in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.30, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Training set : \", X_train.shape)\n",
    "print(\"Shape of test set : \", X_test.shape)\n",
    "print(\"Percentage of classes in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"Percentage of classes in test set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation criterion\n",
    "\n",
    "### Model can make wrong predictions as:\n",
    "\n",
    "1. Predicting a customer will take the personal loan but in reality the customer will not take the personal loan - Loss of resources\n",
    "2. Predicting a customer will not take the personal loan but in reality the customer was going to take the personal loan - Loss of opportunity\n",
    "\n",
    "### Which case is more important? \n",
    "* Losing a potential customer by predicting that the customer will not be taking the personal loan but in reality the customer was going to take the personal loan.\n",
    "\n",
    "### How to reduce this loss i.e need to reduce False Negatives?\n",
    "\n",
    "* Bank would want `Recall` to be maximized, greater the Recall higher the chances of minimizing false negatives. Hence, the focus should be on increasing Recall or minimizing the false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's create functions to calculate different metrics and confusion matrix so that we don't have to use the same code repeatedly for each model.\n",
    "* The model_performance_classification_sklearn_with_threshold function will be used to check the model performance of models. \n",
    "* The confusion_matrix_sklearn_with_threshold function will be used to plot confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
    "\n",
    "\n",
    "def model_performance_classification_sklearn_with_threshold(\n",
    "    model, predictors, target, threshold=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics, based on the threshold specified, to check classification model performance\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    threshold: threshold for classifying the observation as class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred_prob = model.predict_proba(predictors)[:, 1]\n",
    "    pred_thres = pred_prob > threshold\n",
    "    pred = np.round(pred_thres)\n",
    "\n",
    "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
    "    recall = recall_score(target, pred)  # to compute Recall\n",
    "    precision = precision_score(target, pred)  # to compute Precision\n",
    "    f1 = f1_score(target, pred)  # to compute F1-score\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to plot the confusion_matrix of a classification model built using sklearn\n",
    "def confusion_matrix_sklearn_with_threshold(model, predictors, target, threshold=0.5):\n",
    "    \"\"\"\n",
    "    To plot the confusion_matrix, based on the threshold specified, with percentages\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    threshold: threshold for classifying the observation as class 1\n",
    "    \"\"\"\n",
    "    pred_prob = model.predict_proba(predictors)[:, 1]\n",
    "    pred_thres = pred_prob > threshold\n",
    "    y_pred = np.round(pred_thres)\n",
    "\n",
    "    cm = confusion_matrix(target, y_pred)\n",
    "    labels = np.asarray(\n",
    "        [\n",
    "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
    "            for item in cm.flatten()\n",
    "        ]\n",
    "    ).reshape(2, 2)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are different solvers available in Sklearn logistic regression\n",
    "# The newton-cg solver is faster for high-dimensional data\n",
    "\n",
    "lg = LogisticRegression(solver=\"newton-cg\", random_state=1)\n",
    "model = lg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_odds = lg.coef_[0]\n",
    "pd.options.display.float_format = \"{:.5f}\".format\n",
    "pd.DataFrame(log_odds, X_train.columns, columns=[\"coef\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient interpretation\n",
    "\n",
    "* Coefficients of Age, Income, Family, CCAvg, CD_Account, and Education, and some levels of ZIPCode are positive an increase in these will lead to an increase in chances of taking a personal loan.\n",
    "\n",
    "* Coefficients of Securities_Account, CreditCard, Online, and some levels of ZIPCode are negative an increase in these will lead to a decrease in chances of taking a personal loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Converting coefficients to odds\n",
    "* The coefficients of the logistic regression model are in terms of log(odd), to find the odds we have to take the exponential of the coefficients. \n",
    "* Therefore, **odds =  exp(b)**\n",
    "* The percentage change in odds is given as **odds = (exp(b) - 1) * 100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting coefficients to odds\n",
    "odds = np.exp(lg.coef_[0])\n",
    "\n",
    "# finding the percentage change\n",
    "perc_change_odds = (np.exp(lg.coef_[0]) - 1) * 100\n",
    "\n",
    "# removing limit from number of columns to display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# adding the odds to a dataframe\n",
    "pd.DataFrame({\"Odds\": odds, \"Change_odd%\": perc_change_odds}, index=X_train.columns).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient interpretations\n",
    "\n",
    "* `Age`: Holding all other features constant a 1 unit increase in Age will increase the odds of a customer taking the personal loan by 1.004 times or a 0.48% increase in the odds.\n",
    "* `Income`: Holding all other features constant a 1 unit increase in Income will increase the odds of taking a personal loan by 1.05 times or a 5.61% increase in the odds.\n",
    "* `Family`: Holding all other features constant a 1 unit increase in Family will increase the odds of taking a personal loan by 1.88 times or a 88.01% increase in the odds.\n",
    "* `CCAvg`: Holding all other features constant a 1 unit increase in CCAvg will increase the odds of a customer taking a personal loan by 1.20 times or a 20.47% increase in the odds.\n",
    "* `Securities_Account`: The odds of a customer who has a Securities_Account with bank taking a personal loan is 0.45 times or 54.6% less than the customer who doesn't have a Securities_Account.\n",
    "* `Online`: The odds of a customer who prefers internet banking facilities taking a personal loan is 0.58 times or 41.16% less than the customer who doesn't use internet banking facilities.\n",
    "* The odds of a customer from ZIPCode_91 are 0.88 times or 11.16% less than the customer from ZIPCode_90. Similarly, the odds of a customer from ZIPCode_92 are 1.26 times or  26.26% more than the customer from ZIPCode_90. Interpretation of other ZIPCodes can be done in the same way by keeping ZIPCode_90 as the reference.\n",
    "* Education and CD_Account have greater coefficients, so small increases in their value will have a bigger increase in chances of taking a personal loan.\n",
    "\n",
    "`Interpretation for other attributes can be made similarly.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking model performance on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "confusion_matrix_sklearn_with_threshold(lg, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_train_perf = model_performance_classification_sklearn_with_threshold(\n",
    "    lg, X_train, y_train\n",
    ")\n",
    "\n",
    "print(\"Training performance:\")\n",
    "log_reg_model_train_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC-AUC\n",
    "* ROC-AUC on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_roc_auc_train = roc_auc_score(y_train, lg.predict_proba(X_train)[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_train, lg.predict_proba(X_train)[:, 1])\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=\"Logistic Regression (area = %0.2f)\" % logit_roc_auc_train)\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Regression model is giving a good performance on training set.\n",
    "* ROC-AUC score of 0.96 on training is quite good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's see if the recall score can be improved further, by changing the model threshold using AUC-ROC Curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal threshold using AUC-ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal threshold as per AUC-ROC curve\n",
    "# The optimal cut off would be where tpr is high and fpr is low\n",
    "fpr, tpr, thresholds = roc_curve(y_train, lg.predict_proba(X_train)[:, 1])\n",
    "\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold_auc_roc = thresholds[optimal_idx]\n",
    "print(optimal_threshold_auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking model performance on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "confusion_matrix_sklearn_with_threshold(\n",
    "    lg, X_train, y_train, threshold=optimal_threshold_auc_roc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking model performance for this model\n",
    "log_reg_model_train_perf_threshold_auc_roc = model_performance_classification_sklearn_with_threshold(\n",
    "    lg, X_train, y_train, threshold=optimal_threshold_auc_roc\n",
    ")\n",
    "print(\"Training performance:\")\n",
    "log_reg_model_train_perf_threshold_auc_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recall has increased significantly as compared to the previous model.\n",
    "* As we will decrease the threshold value, Recall will keep on increasing and the Precision will decrease, but this not right because it will lead to loss of resources, we need to choose an optimal balance between recall and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use Precision-Recall curve and see if we can find a better threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = lg.predict_proba(X_train)[:, 1]\n",
    "prec, rec, tre = precision_recall_curve(y_train, y_scores,)\n",
    "\n",
    "\n",
    "def plot_prec_recall_vs_tresh(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_prec_recall_vs_tresh(prec, rec, tre)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* At threshold around 0.27 we get a higher recall and a good precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the threshold\n",
    "optimal_threshold_curve = 0.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking model performance on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "confusion_matrix_sklearn_with_threshold(\n",
    "    lg, X_train, y_train, threshold=optimal_threshold_curve\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_train_perf_threshold_curve = model_performance_classification_sklearn_with_threshold(\n",
    "    lg, X_train, y_train, threshold=optimal_threshold_curve\n",
    ")\n",
    "print(\"Training performance:\")\n",
    "log_reg_model_train_perf_threshold_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model is performing well on training set.\n",
    "* Model has given a balanced performance, if the bank wishes to maintain a balance between recall and precision this model can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Feature Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential feature selector is present in mlxtend library\n",
    "# !pip install mlxtend to install mlxtent library\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# to plot the performance with addition of each feature\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining X and Y variables\n",
    "X = data.drop([\"Personal_Loan\", \"Experience\"], axis=1)\n",
    "Y = data[[\"Personal_Loan\"]]\n",
    "\n",
    "X = pd.get_dummies(X, columns=[\"ZIPCode\", \"Education\"], drop_first=True)\n",
    "\n",
    "# Splitting data in train and test sets\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X, Y, test_size=0.30, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on train\n",
    "model = LogisticRegression(solver=\"newton-cg\", n_jobs=-1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will first build model with all varaible\n",
    "sfs = SFS(\n",
    "    model,\n",
    "    k_features=17,\n",
    "    forward=True,\n",
    "    floating=False,\n",
    "    scoring=\"recall\",\n",
    "    verbose=2,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "sfs = sfs.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plot_sfs(sfs.get_metric_dict(), kind=\"std_dev\", figsize=(12, 5))\n",
    "plt.ylim([0.4, 1])\n",
    "plt.title(\"Sequential Forward Selection (w. StdDev)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see that performance increases till the 6th feature and then became constant, and the performance increased again after the addition 11th feature. One of the reasons for such an increase can be the interaction of variables.\n",
    "* The decision to choose the k_features now depends on the recall score vs the complexity of the model - with 6 features we are getting a 0.63 recall score and with 11 features we will get a 0.66 recall score.\n",
    "* The increase in recall score is not much significant as we are getting the same values with a less complex model.\n",
    "* So we'll use 6 features only to build our mode but it depends on the business context and use case of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1 = SFS(\n",
    "    model,\n",
    "    k_features=6,\n",
    "    forward=True,\n",
    "    floating=False,\n",
    "    scoring=\"recall\",\n",
    "    verbose=2,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "sfs1 = sfs1.fit(X_train, y_train)\n",
    "\n",
    "fig1 = plot_sfs(sfs1.get_metric_dict(), kind=\"std_dev\", figsize=(10, 5))\n",
    "\n",
    "plt.ylim([0.4, 1])\n",
    "plt.title(\"Sequential Forward Selection (w. StdDev)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding which features are important**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = list(sfs1.k_feature_idx_)\n",
    "print(feat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at best 6 variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2.columns[feat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train2[X_train2.columns[feat_cols]]\n",
    "\n",
    "# Creating new x_test with the same variables that we selected for x_train\n",
    "X_test_final = X_test2[X_train_final.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting logistic regession model\n",
    "\n",
    "logreg = LogisticRegression(solver=\"newton-cg\", random_state=1)\n",
    "\n",
    "logreg.fit(X_train_final, y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the performance on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "confusion_matrix_sklearn_with_threshold(logreg, X_train_final, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_train_perf_SFS = model_performance_classification_sklearn_with_threshold(\n",
    "    logreg, X_train_final, y_train2\n",
    ")\n",
    "\n",
    "print(\"Training performance:\")\n",
    "log_reg_model_train_perf_SFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the performance on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using model with default threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "confusion_matrix_sklearn_with_threshold(lg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_test_perf = model_performance_classification_sklearn_with_threshold(\n",
    "    lg, X_test, y_test\n",
    ")\n",
    "\n",
    "print(\"Test performance:\")\n",
    "log_reg_model_test_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ROC curve on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_roc_auc_test = roc_auc_score(y_test, lg.predict_proba(X_test)[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, lg.predict_proba(X_test)[:, 1])\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=\"Logistic Regression (area = %0.2f)\" % logit_roc_auc_test)\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using model with threshold=0.11** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "confusion_matrix_sklearn_with_threshold(\n",
    "    lg, X_test, y_test, threshold=optimal_threshold_auc_roc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking model performance for this model\n",
    "log_reg_model_test_perf_threshold_auc_roc = model_performance_classification_sklearn_with_threshold(\n",
    "    lg, X_test, y_test, threshold=optimal_threshold_auc_roc\n",
    ")\n",
    "print(\"Test performance:\")\n",
    "log_reg_model_test_perf_threshold_auc_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using model with threshold = 0.27**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "confusion_matrix_sklearn_with_threshold(\n",
    "    lg, X_test, y_test, threshold=optimal_threshold_curve\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_test_perf_threshold_curve = model_performance_classification_sklearn_with_threshold(\n",
    "    lg, X_test, y_test, threshold=optimal_threshold_curve\n",
    ")\n",
    "print(\"Test performance:\")\n",
    "log_reg_model_test_perf_threshold_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using SFS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "confusion_matrix_sklearn_with_threshold(logreg, X_test_final, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_test_perf_SFS = model_performance_classification_sklearn_with_threshold(\n",
    "    logreg, X_test_final, y_test2\n",
    ")\n",
    "\n",
    "print(\"Test performance:\")\n",
    "log_reg_model_test_perf_SFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training performance comparison\n",
    "\n",
    "models_train_comp_df = pd.concat(\n",
    "    [\n",
    "        log_reg_model_train_perf.T,\n",
    "        log_reg_model_train_perf_threshold_auc_roc.T,\n",
    "        log_reg_model_train_perf_threshold_curve.T,\n",
    "        log_reg_model_train_perf_SFS.T,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "models_train_comp_df.columns = [\n",
    "    \"Logistic Regression sklearn\",\n",
    "    \"Logistic Regression-0.11 Threshold\",\n",
    "    \"Logistic Regression-0.27 Threshold\",\n",
    "    \"Logistic Regression - SFS\",\n",
    "]\n",
    "\n",
    "print(\"Training performance comparison:\")\n",
    "models_train_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing performance comparison\n",
    "\n",
    "models_test_comp_df = pd.concat(\n",
    "    [\n",
    "        log_reg_model_test_perf.T,\n",
    "        log_reg_model_test_perf_threshold_auc_roc.T,\n",
    "        log_reg_model_test_perf_threshold_curve.T,\n",
    "        log_reg_model_test_perf_SFS.T,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "models_test_comp_df.columns = [\n",
    "    \"Logistic Regression sklearn\",\n",
    "    \"Logistic Regression-0.11 Threshold\",\n",
    "    \"Logistic Regression-0.27 Threshold\",\n",
    "    \"Logistic Regression - SFS\",\n",
    "]\n",
    "\n",
    "print(\"Test set performance comparison:\")\n",
    "models_test_comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- We have been able to build a predictive model that can be used by the bank to find the potential customers who will be willing to take a personal loan with recall of 0.89 on the training set and formulate marketing policies accordingly.\n",
    "\n",
    "* The logistic regression models are giving a generalized performance on training and test set.\n",
    "\n",
    "*  Using the model with default threshold the model will give a low recall but good precision score - This model will help the bank save resources but lose on potential customers.\n",
    "* Using the model with 0.11 threshold the model will give a high recall but low precision score - This model will help the bank identify potential customers effectively but the cost of resources will be high.\n",
    "* Using the model with 0.27 threshold the model will give a balance recall and precision score - This model will help the bank to maintain a balance in identifying potential customer and the cost of resources.\n",
    "* The model obtained after SFS gives a similar performance as initial model but with less number of variables.\n",
    "\n",
    "* Coefficients of Age, Income, Family, CCAvg, CD_Account, and Education, and some levels of ZIPCode are positive an increase in these will lead to an increase in chances of taking a personal loan.\n",
    "\n",
    "* Coefficients of Securities_Account, CreditCard, Online, and some levels of ZIPCode are negative an increase in these will lead to a decrease in chances of taking a personal loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's create functions to calculate different metrics and confusion matrix so that we don't have to use the same code repeatedly for each model.\n",
    "* The model_performance_classification_sklearn function will be used to check the model performance of models. \n",
    "* The confusion_matrix_sklearnfunction will be used to plot confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
    "def model_performance_classification_sklearn(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check classification model performance\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
    "    recall = recall_score(target, pred)  # to compute Recall\n",
    "    precision = precision_score(target, pred)  # to compute Precision\n",
    "    f1 = f1_score(target, pred)  # to compute F1-score\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_sklearn(model, predictors, target):\n",
    "    \"\"\"\n",
    "    To plot the confusion_matrix with percentages\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(predictors)\n",
    "    cm = confusion_matrix(target, y_pred)\n",
    "    labels = np.asarray(\n",
    "        [\n",
    "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
    "            for item in cm.flatten()\n",
    "        ]\n",
    "    ).reshape(2, 2)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"gini\", random_state=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking model performance on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_perf_train = model_performance_classification_sklearn(\n",
    "    model, X_train, y_train\n",
    ")\n",
    "decision_tree_perf_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0 errors on the training set, each sample has been classified correctly.\n",
    "* Model has performed very well on training set.\n",
    "* As we know a decision tree will continue to grow and classify each data point correctly if no restrictions are applied as the trees will learn all the patterns in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(X_train.columns)\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 30))\n",
    "out = tree.plot_tree(\n",
    "    model,\n",
    "    feature_names=feature_names,\n",
    "    filled=True,\n",
    "    fontsize=9,\n",
    "    node_ids=False,\n",
    "    class_names=None,\n",
    ")\n",
    "# below code will add arrows to the decision tree split if they are missing\n",
    "for o in out:\n",
    "    arrow = o.arrow_patch\n",
    "    if arrow is not None:\n",
    "        arrow.set_edgecolor(\"black\")\n",
    "        arrow.set_linewidth(1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text report showing the rules of a decision tree -\n",
    "\n",
    "print(tree.export_text(model, feature_names=feature_names, show_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance of features in the tree building ( The importance of a feature is computed as the\n",
    "# (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n",
    "\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        model.feature_importances_, columns=[\"Imp\"], index=X_train.columns\n",
    "    ).sort_values(by=\"Imp\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Income is the most important feature followed by family and education.\n",
    "* The tree above is very complex and difficult to interpret. \n",
    "* Let's prune the tree to see if we can reduce the complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Pre-Pruning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of classifier.\n",
    "estimator = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {\n",
    "    \"max_depth\": np.arange(6, 15),\n",
    "    \"min_samples_leaf\": [1, 2, 5, 7, 10],\n",
    "    \"max_leaf_nodes\": [2, 3, 5, 10],\n",
    "}\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "estimator = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking performance on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(estimator, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_tune_perf_train = model_performance_classification_sklearn(\n",
    "    estimator, X_train, y_train\n",
    ")\n",
    "decision_tree_tune_perf_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "out = tree.plot_tree(\n",
    "    estimator,\n",
    "    feature_names=feature_names,\n",
    "    filled=True,\n",
    "    fontsize=9,\n",
    "    node_ids=False,\n",
    "    class_names=None,\n",
    ")\n",
    "# below code will add arrows to the decision tree split if they are missing\n",
    "for o in out:\n",
    "    arrow = o.arrow_patch\n",
    "    if arrow is not None:\n",
    "        arrow.set_edgecolor(\"black\")\n",
    "        arrow.set_linewidth(1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text report showing the rules of a decision tree -\n",
    "\n",
    "print(tree.export_text(estimator, feature_names=feature_names, show_weights=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "* We can see that the tree has become simpler and more readable.\n",
    "* The model performance has decreased but a recall of 0.87 is still satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance of features in the tree building ( The importance of a feature is computed as the\n",
    "# (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n",
    "\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        model.feature_importances_, columns=[\"Imp\"], index=X_train.columns\n",
    "    ).sort_values(by=\"Imp\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = estimator.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision tree after pre-pruning has given similar feature importance and decision rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from decision rules**\n",
    "* Online, CreditCard, Securities_Account, ZIPCode have very little importance, Income is most important followed by Family and Education\n",
    "* People with Income less than 116.5k dollars, CCAvg less than 2.95, and Income less than 106.5k dollars have fewer chances of taking a Personal Loan.\n",
    "* People having income more than 116.5, family size less than or equal to 2 and educational level of undergraduate have less chances of taking a loan.\n",
    "* People having income more than 116.5, family size more than 2 and educational level higher than undergraduate have more chances of taking a loan.\n",
    "* So bank should campaign more on people with higher income, More education, and larger family sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Cost Complexity Pruning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a decision tree using effective alphas. The last value\n",
    "in ``ccp_alphas`` is the alpha value that prunes the whole tree,\n",
    "leaving the tree, ``clfs[-1]``, with one node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)\n",
    "print(\n",
    "    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "        clfs[-1].tree_.node_count, ccp_alphas[-1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder, we remove the last element in\n",
    "``clfs`` and ``ccp_alphas``, because it is the trivial tree with only one\n",
    "node. Here we show that the number of nodes and tree depth decreases as alpha\n",
    "increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = clfs[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "node_counts = [clf.tree_.node_count for clf in clfs]\n",
    "depth = [clf.tree_.max_depth for clf in clfs]\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 7))\n",
    "ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha\")\n",
    "ax[0].set_ylabel(\"number of nodes\")\n",
    "ax[0].set_title(\"Number of nodes vs alpha\")\n",
    "ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha\")\n",
    "ax[1].set_ylabel(\"depth of tree\")\n",
    "ax[1].set_title(\"Depth vs alpha\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall vs alpha for training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_train = []\n",
    "for clf in clfs:\n",
    "    pred_train = clf.predict(X_train)\n",
    "    values_train = recall_score(y_train, pred_train)\n",
    "    recall_train.append(values_train)\n",
    "\n",
    "recall_test = []\n",
    "for clf in clfs:\n",
    "    pred_test = clf.predict(X_test)\n",
    "    values_test = recall_score(y_test, pred_test)\n",
    "    recall_test.append(values_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"Recall\")\n",
    "ax.set_title(\"Recall vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, recall_train, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, recall_test, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_best_model = np.argmax(recall_test)\n",
    "best_model = clfs[index_best_model]\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Post-pruning using ccp alpha returns the same model as the initial model (Tree with no pruning).\n",
    "* As post pruning model is the same as the initial decision tree mode, the performance and feature importance will also be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the performance on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the decision tree with default parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_perf_test = model_performance_classification_sklearn(\n",
    "    model, X_test, y_test\n",
    ")\n",
    "decision_tree_perf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the hyperparameter tuned decision tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(estimator, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_tune_perf_test = model_performance_classification_sklearn(\n",
    "    estimator, X_test, y_test\n",
    ")\n",
    "decision_tree_tune_perf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Decision Tree models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training performance comparison\n",
    "\n",
    "models_train_comp_df = pd.concat(\n",
    "    [decision_tree_perf_train.T, decision_tree_tune_perf_train.T], axis=1,\n",
    ")\n",
    "models_train_comp_df.columns = [\"Decision Tree sklearn\", \"Decision Tree (Pre-Pruning)\"]\n",
    "print(\"Training performance comparison:\")\n",
    "models_train_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing performance comparison\n",
    "\n",
    "models_test_comp_df = pd.concat(\n",
    "    [decision_tree_perf_test.T, decision_tree_tune_perf_test.T], axis=1,\n",
    ")\n",
    "models_test_comp_df.columns = [\"Decision Tree sklearn\", \"Decision Tree (Pre-Pruning)\"]\n",
    "print(\"Test set performance comparison:\")\n",
    "models_test_comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "* Overall we can see that the Decision tree performs better on the dataset\n",
    "* Looking at important variables based on p-values in Logistic regression and Feature importance in Decision trees\n",
    "    * Income, CCAvg, CD_Account, Family, Education are important in Both\n",
    "    * From the Logistic Regression model we observe that the above-mentioned attributes have a positive relationship with Personal Loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Recommendations\n",
    "\n",
    "* We have been able to build a predictive model:\n",
    "\n",
    "  a) that the bank can deploy to identify customers who will be interested in taking a personal loan.\n",
    "\n",
    "  b) that the bank can use to find the key factors that will have an impact on a customer taking a personal loan or not.\n",
    "\n",
    "* Factors that have an impact on Personal_Loan: Income, Family, Education. \n",
    "\n",
    "* Higher income customers should be the target customers for the bank - Customers who have income above 116k dollars and a family of more than 2, such customers have higher chances of taking personal loans.\n",
    "\n",
    "* Higher education higher are the chances to take a loan - Customers who are more educated (education level greater than undergraduate) have a higher chance of taking a personal loan.\n",
    "\n",
    "* Size of the family has a positive correlation with the personal loan, as the size of the family increases (generally a family size of 3 or more than 3 members) have more chances of a customer taking a personal loan.\n",
    "\n",
    "* Our analysis showed that ~50% of the customers who have the certificate of deposit with the bank (CD_Account) are the ones that have the requirement of Personal Loan - Bank should target such customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassification analysis (Additional)\n",
    "* To check whether there is any certain pattern followed by samples that are incorrectly classified by our model (dTree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ZIPCode\"] = data[\"ZIPCode\"].astype(\"category\")\n",
    "\n",
    "X = data.drop([\"Personal_Loan\", \"Experience\"], axis=1)\n",
    "Y = data[[\"Personal_Loan\"]]\n",
    "\n",
    "X = pd.get_dummies(X, columns=[\"ZIPCode\", \"Education\"], drop_first=True)\n",
    "\n",
    "# Splitting data in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.30, random_state=1\n",
    ")\n",
    "Y1 = model.predict(X_test)\n",
    "Y1 = Y1.reshape(1500, 1)\n",
    "\n",
    "Y2 = np.subtract(y_test.astype(\"int\"), Y1)\n",
    "\n",
    "# 1 says, Perosn would buy loan but model predicted he won't\n",
    "# -1 says, Perosn won't buy loan but model predicted he would\n",
    "\n",
    "# Let's concatenate this Y2 with X\n",
    "data1 = pd.DataFrame(Y2)\n",
    "data2 = pd.concat([X_test, data1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_data = data2[data2[\"Personal_Loan\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 21 misclassifications and on the test set.\n",
    "* *incorrect_data* consists of all misclassified elements.\n",
    "* Let's try to see if there is any specific pattern in these samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profile = ProfileReport(incorrect_data, title=\"Misclassified Data Points\", minimal=True)\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the above profile, we see that incorrectly classified people are :\n",
    "* Usually between 26 and 65 age and have experience in between 2 to 40 years, with 15 and 17 uniques values.\n",
    "* Income varies between 60 to 115(thousand dollars), while usual income varied from 8 to 224(thousand dollars)\n",
    "* Most of the people misclassified have 0 mortgages, no Securities Account, and no CD_account, have a family size 1 or 2, and customers who do not prefer internet banking facilities.\n",
    "* Based on the business rule, we derived we were able to see that usually people with income less than 116, less mortgage, family size less than 3 doesn't buy loan - There are special cases always, so some people with less income and smaller family size might also buy loan."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
